
OUTPUT LAYER:
You must choose the activation function for your output layer based on the type of prediction problem that you are solving.

Specifically, the type of variable that is being predicted.

For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) 
and predicting a numerical variable (regression).

If your problem is a regression problem, you should use a linear activation function.

Regression: One node, linear activation.

->If the model is a regressor then the output layer will have only a single neuron



HIDDEN LAYERS ACTIVATION function:
Multilayer Perceptron (MLP): ReLU activation function.
στην παρουσιαση του μιλαει για συναρτηση που πρεπει να ειναι διαφορισιμη 
η relu ειναι?
=χρησιμοποιώ leaky relu. η relu ειναι διαφορισιμη στο μηδεν θεωρουμε 0 και επιπλεον για να αποφυγω το προβλημα με 
τους νευρωνες πεθαινουν διοτι ειναι 0 για χ<0 χρησιμοποιω τη leaky relu που δεν ειναι ακριβως 0.
oι αλλες συναρτησεις μπορεί να παθουν Κορεσμό
->Η διαφορισιμότητα είναι η μόνη απαίτηση που πρέπει να
ικανοποιεί μια συνάρτηση ενεργοποίησης σε έναν αλγόριθμο
ΠΔ.
Αυτό απαιτείται για τον υπολογισμό του δ για κάθε νευρώνα.
Συνήθως χρησιμοποιούνται σιγμοειδείς συναρτήσεις καθώς
ικανοποιούν μια τέτοια συνθήκη

Υπερβολικής Εφαπτομένης:
Κορεσμός και επιθυμητή απόκριση
Μπορούμε να επιλέξουμε τις τιμές dj έτσι ώστε να απέχουν από τα όρια κορεσμού.
Για +a: dj
=a – ε και για -a: dj
=-a + ε
 Για a=1.7159, ε=0.7159, οπότε (πολύ βολικά) dj=+-1

================================================================================================================
There are many rule-of-thumb methods for determining the correct number of neurons to use in the hidden layers, such as the following:

The number of hidden neurons should be between the size of the input layer and the size of the output layer.
The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.
The number of hidden neurons should be less than twice the size of the input layer.
Most of the problems can be solved by using a single hidden layer with the number of neurons equal to
the mean of the input and output layer. If less number of neurons is chosen it will lead to underfitting 
and high statistical bias. Whereas if we choose too many neurons it may lead to overfitting, high variance, 
and increases the time it takes to train the network.

====================
LOSS FUNCTION:
The choice of loss function depends on the task,
but common ones include mean squared error (MSE) for regression tasks 

======================
Για τη σύγκλισ:
Ο αλγόριθμος ΠΔ θεωρείται ότι έχει συγκλίνει όταν η απόλυτη μεταβολή στο
μέσο τετραγωνικό σφάλμα ανά κύκλο είναι αρκετά μικρή [0.1%,1%]


early stopping:
Αποφυγή υπερεκπαίδευσης: πρόωρο σταμάτημα (early stopping)
 Εκπαιδεύουμε το MLP (ενημερώνουμε τα βάρη του) μέσω της
ελαχιστοποίησης του σφάλματος εκπαίδευσης.
 Σε τακτά χρονικά διαστήματα (π.χ. κάθε 10 εποχές)
‘παγώνουμε’ τη διαδικασία εκπαίδευσης και με τις
τρέχουσες τιμές των βαρών υπολογίζουμε μια εκτίμηση του
σφάλματος γενίκευσης σε ένα ανεξάρτητο σύνολο
παραδειγμάτων (διαφορετικό από το σύνολο εκπαίδευσης και
το σύνολο ελέγχου).
 Το τρίτο αυτό σύνολο παραδειγμάτων που χρησιμοποιούμε
ονομάζεται σύνολο επικύρωσης (validation set) και το
αντίστοιχο σφάλμα ονομάζεται σφάλμα επικύρωσης.
 Κατόπιν συνεχίζουμε τη διαδικασία εκπαίδευσης και της
ενημέρωσης των βαρών μέχρι το επόμενο χρονικό σημείο
υπολογισμού του σφάλματος επικύρωσης.

Τίμημα: θα πρέπει να αφαιρέσουμε ένα ποσοστό των
παραδειγμάτων από το σύνολο εκπαίδευσης και να τα
βάλουμε στο σύνολο επικύρωσης. Πρόβλημα εάν τα
παραδείγματα είναι λίγα. Εξάρτηση από τον διαμερισμό.
 Δεν επιτρέπεται τα σύνολα εκπαίδευσης, επικύρωσης και
ελέγχου να έχουν κοινά παραδείγματα.
=======================
ΟΡΜΗ:
Μικρό η: ομοιόμορφη τροχιά, ακρίβεια, αλλά αργή σύγκλιση
Μεγάλο η: Επιτάχυνση, αλλά μεγάλες αλλαγές, ταλαντώσεις, αστάθεια
->Μια απλή μέθοδος για την αύξηση της ταχύτητας μάθησης και την αποφυγή
αστάθειας(για υψηλό ρυθμό μάθησης η) είναι η τροποποίηση του κανόνα
δέλτα λαμβάνοντας υπόψη έναν όρο ορμής ως εξής:
όπου α είναι συνήθως ένας σταθερός
αριθμός που καλείται σταθερά ορμής (momentum constant).

Για να εξασφαλιστεί η σύγκλιση, η σταθερά ορμής
πρέπει να είναι μεταξύ: 0 <= |a| < 1, ώστε οι παλιότερες
μεταβολές να βαρύνουν λιγότερο

ο ορος στον οποιο εφαρμοζεται η ορμη αφορα τις παλιοτερες τιμες και πρεπει να εχουν μικροτερη επιδραση

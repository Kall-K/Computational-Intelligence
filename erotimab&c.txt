
OUTPUT LAYER:
You must choose the activation function for your output layer based on the type of prediction problem that you are solving.

Specifically, the type of variable that is being predicted.

For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) 
and predicting a numerical variable (regression).

If your problem is a regression problem, you should use a linear activation function.

Regression: One node, linear activation.

->If the model is a regressor then the output layer will have only a single neuron



HIDDEN LAYERS ACTIVATION function:
Multilayer Perceptron (MLP): ReLU activation function.
στην παρουσιαση του μιλαει για συναρτηση που πρεπει να ειναι διαφορισιμη 
η relu ειναι?
=χρησιμοποιώ leaky relu. η relu ειναι διαφορισιμη στο μηδεν θεωρουμε 0 και επιπλεον για να αποφυγω το προβλημα με 
τους νευρωνες πεθαινουν διοτι ειναι 0 για χ<0 χρησιμοποιω τη leaky relu που δεν ειναι ακριβως 0.
oι αλλες συναρτησεις μπορεί να παθουν Κορεσμό
->Η διαφορισιμότητα είναι η μόνη απαίτηση που πρέπει να
ικανοποιεί μια συνάρτηση ενεργοποίησης σε έναν αλγόριθμο
ΠΔ.
Αυτό απαιτείται για τον υπολογισμό του δ για κάθε νευρώνα.
Συνήθως χρησιμοποιούνται σιγμοειδείς συναρτήσεις καθώς
ικανοποιούν μια τέτοια συνθήκη

Υπερβολικής Εφαπτομένης:
Κορεσμός και επιθυμητή απόκριση
Μπορούμε να επιλέξουμε τις τιμές dj έτσι ώστε να απέχουν από τα όρια κορεσμού.
Για +a: dj
=a – ε και για -a: dj
=-a + ε
 Για a=1.7159, ε=0.7159, οπότε (πολύ βολικά) dj=+-1

================================================================================================================
There are many rule-of-thumb methods for determining the correct number of neurons to use in the hidden layers, such as the following:

The number of hidden neurons should be between the size of the input layer and the size of the output layer.
The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.
The number of hidden neurons should be less than twice the size of the input layer.
Most of the problems can be solved by using a single hidden layer with the number of neurons equal to
the mean of the input and output layer. If less number of neurons is chosen it will lead to underfitting 
and high statistical bias. Whereas if we choose too many neurons it may lead to overfitting, high variance, 
and increases the time it takes to train the network.

====================
LOSS FUNCTION:
The choice of loss function depends on the task,
but common ones include mean squared error (MSE) for regression tasks 

======================
Για τη σύγκλισ:
Ο αλγόριθμος ΠΔ θεωρείται ότι έχει συγκλίνει όταν η απόλυτη μεταβολή στο
μέσο τετραγωνικό σφάλμα ανά κύκλο είναι αρκετά μικρή [0.1%,1%]

=======================
TESTS 
nodes=10000
Fold : 0  RMSE: 0.06872454285621643
Fold : 1  RMSE: 0.08074723929166794
Fold : 2  RMSE: 0.09876656532287598
Fold : 3  RMSE: 0.07701927423477173
Fold : 4  RMSE: 0.06286274641752243
RMSE:  0.0776240736246109
nodes=1000
Fold : 0  RMSE: 0.05958186089992523
Fold : 1  RMSE: 0.08465948700904846
Fold : 2  RMSE: 0.06956696510314941
Fold : 3  RMSE: 0.0795632153749466
Fold : 4  RMSE: 0.09258253872394562
RMSE:  0.07719081342220306
nodes=100
Fold : 0  RMSE: 0.06349924951791763
Fold : 1  RMSE: 0.0528097003698349
Fold : 2  RMSE: 0.09046889841556549
Fold : 3  RMSE: 0.08718524128198624
Fold : 4  RMSE: 0.09130550920963287
RMSE:  0.07705371975898744
==========================